{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7c1d61a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text-hammer in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: TextBlob in c:\\users\\asus\\anaconda3\\lib\\site-packages (from text-hammer) (0.17.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from text-hammer) (1.4.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from text-hammer) (1.21.5)\n",
      "Requirement already satisfied: beautifulsoup4==4.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from text-hammer) (4.9.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from text-hammer) (3.5.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from beautifulsoup4==4.9.1->text-hammer) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->text-hammer) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->text-hammer) (2.8.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (5.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (2.4.6)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (21.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (8.1.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (2.11.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (0.7.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (63.4.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (1.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (2.28.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy->text-hammer) (1.0.9)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from TextBlob->text-hammer) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob->text-hammer) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob->text-hammer) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob->text-hammer) (8.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy->text-hammer) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy->text-hammer) (4.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->text-hammer) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->text-hammer) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->text-hammer) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->text-hammer) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->text-hammer) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->text-hammer) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->text-hammer) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->text-hammer) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy->text-hammer) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install text-hammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3430e9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\asus\\anaconda3\\lib\\site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c0a31c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a24b066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# import imblearn\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f140669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae191a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8675\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mbti_1.csv')\n",
    "#df1=df[]\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6a5a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       INFJ\n",
      "1       ENTP\n",
      "2       INTP\n",
      "3       INTJ\n",
      "4       ENTJ\n",
      "        ... \n",
      "8670    ISFP\n",
      "8671    ENFP\n",
      "8672    INTP\n",
      "8673    INFP\n",
      "8674    INFP\n",
      "Name: type, Length: 8675, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df.reset_index(inplace=True)\n",
    "df.head()\n",
    "print(df.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9aff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_hammer as th\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def text_preprocessing(df,col_name):\n",
    "    column = col_name\n",
    "    type(df[column])\n",
    "    df[column] = df[column].progress_apply(lambda x:str(x).lower())\n",
    "    print(type(df[column]))\n",
    "    print(1)\n",
    "    df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) #you're -> you are; i'm -> i am\n",
    "    print(type(df[column]))\n",
    "    print(2)\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n",
    "    print(type(df[column]))\n",
    "    print(3)\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n",
    "    print(type(df[column]))\n",
    "    print(4)\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_urls(x))\n",
    "    print(df[column])\n",
    "    print(4)\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_stopwords(x))\n",
    "    print(type(df[column]))\n",
    "    print(5)\n",
    "    #df[column] = df[column].progress_apply(lambda x:th.spelling_correction(x))\n",
    "    #print(type(df[column]))\n",
    "    print(6)\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(str(x)))\n",
    "    print(type(df[column]))\n",
    "    df[column]=pd.Series(df[column])\n",
    "    print(7)\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n",
    "    print(type(df[column]))\n",
    "    print(8)\n",
    "    df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,\n",
    "    print(type(df[column]))\n",
    "    print(9)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aec437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f660436dd09342dcae3bc4f5c083dc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b36f59c127346ffab4a86b20468f943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df=text_preprocessing(df1,\"posts\")\n",
    "\n",
    "import time\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Your program code goes here\n",
    "df=text_preprocessing(df,\"posts\")\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time taken\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Time taken: {time_taken:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a654620",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)\n",
    "print(df.iloc[:,1])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aede5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:,1].values\n",
    "print(\"before: \\n\",y)\n",
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)\n",
    "y=to_categorical(y)\n",
    "for i in y[5]:\n",
    "    if(i==1):\n",
    "        print(list(y[5]).index(i))\n",
    "        print(len(y[5]))\n",
    "print(\"After: \\n\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.set()\n",
    "# Convert the column to categorical type\n",
    "df.iloc[:, 1] = pd.Categorical(df.iloc[:, 1])\n",
    "t=list(set(df.iloc[:,1]))\n",
    "\n",
    "# Plot the count of each category\n",
    "sns.countplot(x=df.iloc[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e984b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "nltk.download('punkt')\n",
    "words_list = []\n",
    "for post in df1['posts']:\n",
    "    words_list.extend(nltk.word_tokenize(post))\n",
    "freq_dist = nltk.FreqDist(words_list)\n",
    "freq_dist.most_common(20)\n",
    "temp = pd.DataFrame(freq_dist.most_common(30),  columns=['word', 'count'])\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(x='word', y='count', \n",
    "            data=temp, ax=ax)\n",
    "plt.title(\"Top words\")\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import wordcloud\n",
    "# creation of wordcloud\n",
    "wcloud_fig = WordCloud( stopwords=set(wordcloud.STOPWORDS),\n",
    "                      colormap='viridis', width=300, height=200).generate_from_frequencies(freq_dist)\n",
    "\n",
    "# plotting the wordcloud\n",
    "plt.figure(figsize=(10,7), frameon=True)\n",
    "\n",
    "plt.imshow(wcloud_fig, interpolation  = 'bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "# sns.swarmplot(\"type\", \"words_per_comment\", data=df)\n",
    "sns.swarmplot(x=\"type\", y=\"words_per_comment\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6125b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = 50000\n",
    "messages=df['posts'].copy()\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230e81b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "corpus=[]\n",
    "for i in range (0,len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]','',messages[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english') ]\n",
    "    review=''.join(review)\n",
    "    corpus.append(review)\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a359872",
   "metadata": {},
   "outputs": [],
   "source": [
    "oe=[one_hot(words,voc_size) for words in corpus ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c619d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_length=250\n",
    "embedded_docs = pad_sequences(oe,padding='pre',maxlen = sent_length)\n",
    "embedded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3059ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4453c",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4315a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension=100\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size,dimension,input_length = sent_length))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(16,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer ='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b159281",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42) # fit predictor and target variable\n",
    "x_rus, y_rus = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23caeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x_rus, y_rus, test_size=0.2, random_state=42)\n",
    "print((y_train[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50448eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape\n",
    "callbacks = [EarlyStopping( monitor=\"val_accuracy\", patience=2 ), \n",
    "             ModelCheckpoint('model0.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5753e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=5,batch_size=64, callbacks = callbacks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c669fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(len(x_test[0]))\n",
    "preds = model.predict(x_test)\n",
    "#print(preds)\n",
    "#print(y_test)\n",
    "o=np.argmax(preds,axis=1)\n",
    "#for i in o:\n",
    "#    print(t[i])\n",
    "#print(\"Emotion:\",len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea000c",
   "metadata": {},
   "source": [
    "# Giving Input and Getting the Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b30725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'th' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25116\\2470860265.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcont_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#you're -> you are; i'm -> i am\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_emails\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_html_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'th' is not defined"
     ]
    }
   ],
   "source": [
    "s=input()\n",
    "s =str(s).lower()\n",
    "s=th.cont_exp(s) #you're -> you are; i'm -> i am\n",
    "s=th.remove_emails(s)\n",
    "s =th.remove_html_tags(s)\n",
    "s=th.remove_urls(s)\n",
    "s = th.remove_stopwords(s)\n",
    "    #df[column] = df[column].progress_apply(lambda x:th.spelling_correction(x))\n",
    "    #print(type(df[column]))\n",
    "s =th.remove_special_chars(s)\n",
    "s = th.remove_accented_chars(s)\n",
    "s = th.make_base(s) #ran -> run,\n",
    "print(s)\n",
    "words_list = []\n",
    "words_list.extend(nltk.word_tokenize(s))\n",
    "freq_dist = nltk.FreqDist(words_list)\n",
    "freq_dist.most_common(20)\n",
    "temp = pd.DataFrame(freq_dist.most_common(30),  columns=['word', 'count'])\n",
    "#fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#sns.barplot(x='word', y='count', data=temp, ax=ax)\n",
    "ps=PorterStemmer()\n",
    "corpus=[]\n",
    "review = re.sub('[^a-zA-Z]','',s)\n",
    "review=review.lower()\n",
    "review=review.split()\n",
    "review = [ps.stem(word) for word in review if not word in stopwords.words('english') ]\n",
    "review=''.join(review)\n",
    "corpus.append(review)\n",
    "oe=[one_hot(words,voc_size) for words in corpus ]\n",
    "sent_length=250\n",
    "embedded_docs = pad_sequences(oe,padding='pre',maxlen = sent_length)\n",
    "embedded_docs\n",
    "X=np.array(embedded_docs)\n",
    "p=model.predict(X)\n",
    "o1=np.argmax(p,axis=1)\n",
    "print(\"Personality:\",t[o1[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345650e",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6abcad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[375   1   0   0   0   0   0   0   3   1   1   0   0   1   0   3]\n",
      " [  0 301   5   4   0   0   0   0   2   4   4   5   0   0   0   4]\n",
      " [  0   3 386   2   0   0   0   0   0   2   5   1   0   0   0   0]\n",
      " [  0   0   0 285   0   0   0   0  10   6   3   4   0   0   0   0]\n",
      " [  0   0   0   0 382   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 371   0   0   0   0   0   0   0   0   1   0]\n",
      " [  0   1   0   0   0   0 351   0   0   0   0   1   0   0   0   0]\n",
      " [  0   1   0   1   0   0   0 391   0   0   0   0   0   1   0   3]\n",
      " [  0   0   0   3   0   0   0   0 105   6   6   5   0   0   0   0]\n",
      " [  0  19   2  15   0   0   0   0  80 138  55  73   0   0   0   2]\n",
      " [  1  27   0  26   0   0   0   0 129 145 278  83   0   2   1   3]\n",
      " [  0   4   0   2   0   0   0   0  26  40  15 168   0   0   0   2]\n",
      " [  0   3   1   2   0   0   0   0   1   0   2   0 348   0   0   0]\n",
      " [  0   0   3   2   0   0   0   0   1   1   4   1   2 382   0   3]\n",
      " [  0   0   4   1   0   0   0   0   1   0   1   1   0   0 351   1]\n",
      " [  0   0   0   1   0   0   0   0   2   2   5   2   0   1   0 348]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "true_cat = []\n",
    "for y in y_test:\n",
    "     true_cat.append(np.where(y==1)[0])\n",
    "predicted_cat = tf.argmax(preds, axis=1)\n",
    "predicted_cat\n",
    "print(confusion_matrix(predicted_cat, true_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36107497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
